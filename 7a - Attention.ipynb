{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "README -- Out of memory errors <br>\n",
    "This code is tuned for a RTX3060 12GB, if you have less memory, you will likely get \"out of memory\" errors with this code, try to aggressively reduce batch size (you can increase epochs to compensate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Parallel Approach to Sequence Modeling\n",
    "\n",
    "By leveraging the sequence length as a batch dimension, applying a linear layer to all embeddings on the GPU becomes highly efficient. In the attention mechanism, this process is repeated three times, yielding: \n",
    "\n",
    "**Keys**, **Queries**, **Values** -- Each of these has the shape `(batch_size, sequence_length, embedding_size)`.\n",
    "\n",
    "We then multiply the keys and queries (transposed), resulting in a similarity matrix of size `(batch_size, sequence_length, sequence_length)`. This matrix indicates how much each token attends to (or is interested in) every other token. Subsequently, we multiply this matrix by the values and sum the results.\n",
    "\n",
    "The core idea is that by constructing a similarity matrix, we can identify dependencies and use the values to update words with their new semantic meanings. For example, the word \"apple\" can refer to both a fruit and a technology company, the attention mechanism allows its meaning to be inferred from other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import LucaM185 \n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "with open(\"datasets/enwik8\", \"r\") as f:\n",
    "    load = f.read()\n",
    "print(load[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyInputs:\n",
    "    def __init__(self, data, sequence_length):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        start_indices = idx.unsqueeze(1) + torch.arange(self.sequence_length)\n",
    "        return self.data[start_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restricting to ASCII characters\n",
    "texttoint = {chr(i): i for i in range(256)}\n",
    "inttotext = {i: chr(i) for i in range(256)}\n",
    "dataset = torch.tensor([texttoint[c] for c in load if c in texttoint])\n",
    "\n",
    "vocab_size = len(texttoint)\n",
    "sequence_length = 20\n",
    "val_set = int(len(dataset)*0.1)\n",
    "\n",
    "inputs = MyInputs(dataset[val_set:-1-sequence_length], sequence_length)\n",
    "labels = MyInputs(dataset[val_set+1:-sequence_length], sequence_length)\n",
    "val_inputs = MyInputs(dataset[:val_set], sequence_length)\n",
    "val_labels = MyInputs(dataset[1:val_set+1], sequence_length)\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, n_layers):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.fc_in = nn.Linear(emb_size*sequence_length, hidden_size)\n",
    "        self.fc_hidden = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(n_layers)]) # this is a list of linear layers\n",
    "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        emb = self.embeddings(inputs).view(inputs.shape[0], -1)\n",
    "        x = F.gelu(self.fc_in(emb))\n",
    "        for hidden in self.fc_hidden:    # iterating over hidden layers\n",
    "            x = F.gelu(hidden(x))  # applying each hidden layer\n",
    "        return self.fc_out(x)\n",
    "\n",
    "model = Model(emb_size=32, hidden_size=256, n_layers=2).to(device)\n",
    "print(f\"Millions of parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}\")\n",
    "\n",
    "steps = 300\n",
    "batch_size = 4096\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)  # 3e-4 is a good learning rate for Adam on big models\n",
    "\n",
    "lossi = []\n",
    "vlossi = []\n",
    "\n",
    "for step in range(steps):\n",
    "    indexes = torch.randint(0, len(inputs), (batch_size,))\n",
    "    \n",
    "    pred = model(inputs[indexes].to(device)) \n",
    "    loss = F.cross_entropy(pred, labels[indexes][:, -1].to(device))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        indexes = torch.randint(0, len(val_inputs), (batch_size//8,))\n",
    "        pred = model(val_inputs[indexes].to(device))\n",
    "        vloss = F.cross_entropy(pred, val_labels[indexes][:, -1].to(device))\n",
    "        vlossi.append(vloss.item())\n",
    "        \n",
    "    if step % (steps//10) == 0:\n",
    "        print(f\"Step {step:_>4d} - Loss: {loss.item():.3f} - Val Loss: {vloss.item():.3f}\")\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(6, 3))  \n",
    "plt.plot(lossi, label='Loss')\n",
    "plt.plot(vlossi, label='Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting our dataset to ascii characters only reduces the size by 0.3% so we should be fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Block - Scaled dot product attention\n",
    "\n",
    "Scaled Dot-Product Attention allows a model to focus on different parts of an input sentence by comparing each word to every other word. It works by turning the input sentence into three types of information: **queries (Q)**, **keys (K)**, and **values (V)**, which are all derived from the same sentence. The queries and keys are compared to see which words are most relevant to each other. \n",
    "\n",
    "The model then calculates how much attention each word should give to others, based on these comparisons. To make sure the calculations are stable, we scale the comparisons and then turn them into probabilities. The result is a weighted sum of the values, which helps the model focus on the most important parts of the sentence.\n",
    "\n",
    "This method compares every word with every other word, so it can become slow as the sentence gets longer, requiring more calculations. Actually if we want to use the better way of predicting tokens at each timestep from the RNN, we need to make sure that in the attention mechanism, past tokens can't get information from future ones, we do this by zeroing out the values with a tril matrix like this:\n",
    "\n",
    "\n",
    "```\n",
    "[ 1,  0,  0,  0 ]\n",
    "[ 1,  1,  0,  0 ]\n",
    "[ 1,  1,  1,  0 ]\n",
    "[ 1,  1,  1,  1 ]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):  \n",
    "    def __init__(self, emb_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size) \n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.keys(x)\n",
    "        q = self.queries(x)\n",
    "        v = self.values(x)\n",
    "        similarity = k @ q.transpose(-2, -1)/(self.emb_size**0.5)\n",
    "\n",
    "        # By masking the attention we make sure that current tokens can't see future tokens, this allows us to train\n",
    "        # the model with the entire sequence instead of a single token at a time. We call this \"casual\" attention\n",
    "        similarity[torch.tril(torch.ones_like(similarity)) == 0] = float(\"-inf\")\n",
    "        similarity = torch.softmax(similarity, dim = -1)\n",
    "\n",
    "        attention = similarity @ v \n",
    "        return attention\n",
    "\n",
    "\n",
    "class FullyConnected(nn.Module):  \n",
    "    def __init__(self, in_size, out_size, hidden_size, n_layers):\n",
    "        super(FullyConnected, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_size, hidden_size)\n",
    "        self.fcx = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(n_layers)])\n",
    "        self.fc2 = nn.Linear(hidden_size, out_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        for fc in self.fcx:\n",
    "            x = F.gelu(fc(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wrapper(nn.Module): \n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, n_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.posemb = nn.Embedding(sequence_length, emb_size)\n",
    "\n",
    "        self.prenorm1 = nn.LayerNorm(emb_size)\n",
    "        self.att = Attention(emb_size)\n",
    "        self.prenorm2 = nn.LayerNorm(emb_size)\n",
    "        self.fc = FullyConnected(emb_size, emb_size, hidden_size, n_layers)\n",
    "\n",
    "        self.fcout = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):          \n",
    "        x = self.embedding(x) + self.posemb(torch.arange(x.size(1)).to(x.device)) \n",
    "        x = x + self.att(self.prenorm1(x)) \n",
    "        x = x + self.fc(self.prenorm2(x))\n",
    "        x = self.fcout(x)\n",
    "    \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = Wrapper(vocab_size=vocab_size, emb_size=256, hidden_size=256, n_layers=2).to(device)\n",
    "torch.compile(model)\n",
    "print(f\"Millions of parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}\")\n",
    "\n",
    "steps = 600\n",
    "batch_size = 512\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4) \n",
    "\n",
    "lossi = []\n",
    "vlossi = []\n",
    "\n",
    "for step in range(steps):\n",
    "    indexes = torch.randint(0, len(inputs), (batch_size,))\n",
    "    \n",
    "    pred = model(inputs[indexes].to(device)) # everything in the forward pass happens in the model class\n",
    "    loss = F.cross_entropy(pred.view(-1, vocab_size), labels[indexes].to(device).view(-1))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "    with torch.no_grad():\n",
    "        indexes = torch.randint(0, len(val_inputs), (batch_size//8,))\n",
    "        pred = model(val_inputs[indexes].to(device))\n",
    "\n",
    "        loss = F.cross_entropy(pred.view(-1, vocab_size), val_labels[indexes].to(device).view(-1))\n",
    "        vlossi.append(loss.item())\n",
    "\n",
    "    if step % (steps//10) == 0:\n",
    "        print(f\"Step {step:_>4d} - Loss: {lossi[-1]:.3f} - Val Loss: {vlossi[-1]:.3f}\")\n",
    "    \n",
    "plt.figure(figsize=(6, 3))  \n",
    "plt.plot(vlossi[100:], label='Validation Loss')\n",
    "plt.plot(lossi[100:], label='Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performance is slightly worse than 20 layers MLP, but we are using just a single block (attention + 2 hidden layer MLP)  <br>\n",
    "Note: we are using roughly 20k parameters instead of 370k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '\\n<mediawiki xmlns=\"http://www.media'\n",
    "print(string, end=\"\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(300):\n",
    "        X = torch.tensor([texttoint[s] for s in string[-sequence_length:]]).long().view(1, -1).to(device)\n",
    "        pred = model.forward(X)\n",
    "        string += inttotext[torch.multinomial(F.softmax(pred[0, -1, :], dim=0), 1).item()]\n",
    "        print(string[-1], end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers \n",
    "\n",
    "Transformers consist of many attention blocks and MLPs in sequence, with residual connections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module): \n",
    "    def __init__(self, emb_size, hidden_size, n_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.prenorm1 = nn.LayerNorm(emb_size)\n",
    "        self.att = Attention(emb_size)\n",
    "        self.prenorm2 = nn.LayerNorm(emb_size)\n",
    "        self.fc = FullyConnected(emb_size, emb_size, hidden_size, n_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.att(self.prenorm1(x)))\n",
    "        x = x + (self.fc(self.prenorm2(x)))\n",
    "    \n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module): \n",
    "    def __init__(self, vocab_size, emb_size, n_blocks, head_size, hidden_size, n_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.posemb = nn.Embedding(sequence_length, emb_size)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([Block(emb_size, hidden_size, n_layers) for _ in range(n_blocks)])\n",
    "        self.LinOut = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (self.embedding(x) + self.posemb(torch.arange(x.size(1)).to(x.device))) \n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.LinOut(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model = Transformer(vocab_size=vocab_size, emb_size=256, n_blocks=4, head_size=256, hidden_size=256, n_layers=2).to(device)\n",
    "torch.compile(model)\n",
    "print(f\"Millions of parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}\")\n",
    "\n",
    "steps = 400 # Tailored to be time-comparable to the MLP\n",
    "batch_size = 512\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.03) \n",
    "\n",
    "lossi = []\n",
    "vlossi = []\n",
    "\n",
    "for step in range(steps):\n",
    "    model.train()\n",
    "    indexes = torch.randint(0, len(inputs), (batch_size,))\n",
    "    \n",
    "    pred = model(inputs[indexes].to(device)) # everything in the forward pass happens in the model class\n",
    "    loss = F.cross_entropy(pred.view(-1, vocab_size), labels[indexes].to(device).view(-1))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        indexes = torch.randint(0, len(val_inputs), (batch_size//8,))\n",
    "        pred = model(val_inputs[indexes].to(device))\n",
    "        loss = F.cross_entropy(pred.view(-1, vocab_size), val_labels[indexes].to(device).view(-1))\n",
    "        vlossi.append(loss.item())\n",
    "\n",
    "    if step % (steps//10) == 0:\n",
    "        print(f\"Step {step:_>4d} - Loss: {lossi[-1]:.3f} - Val Loss: {vlossi[-1]:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 3))  \n",
    "plt.plot(vlossi[100:], label='Validation Loss')\n",
    "plt.plot(lossi[100:], label='Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '\\n<mediawiki xmlns=\"http://www.media'\n",
    "print(string, end=\"\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(300):\n",
    "        X = torch.tensor([texttoint[s] for s in string[-sequence_length:]]).long().view(1, -1).to(device)\n",
    "        pred = model.forward(X)\n",
    "        string += inttotext[torch.multinomial(F.softmax(pred[0, -1, :], dim=0), 1).item()]\n",
    "        print(string[-1], end=\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed\n",
    "Attention feels pretty slow, because it's quadratic in complexity, but if you think about it, for large N, (N-1)^2 of the similarity matrix is the same as the one in t-1. So we only need roughly 2*N computations to complete the matrix. \n",
    "\n",
    "Furthermore since we use self attention with a mask, roughly 50% of the matrix is useless... So we only need N computations to do the incremental step in inference to add a new token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module): \n",
    "    def __init__(self, emb_size, hidden_size, n_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.prenorm1 = nn.LayerNorm(emb_size)\n",
    "        self.att = Attention(emb_size)\n",
    "        self.prenorm2 = nn.LayerNorm(emb_size)\n",
    "        self.fc = FullyConnected(emb_size, emb_size, hidden_size, n_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.att(self.prenorm1(x)))\n",
    "        x = x + (self.fc(self.prenorm2(x)))\n",
    "    \n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module): \n",
    "    def __init__(self, vocab_size, emb_size, n_blocks, head_size, hidden_size, n_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        #self.posemb = nn.Embedding(sequence_length, emb_size)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([Block(emb_size, hidden_size, n_layers) for _ in range(n_blocks)])\n",
    "        self.LinOut = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (self.embedding(x) + self.posemb(torch.arange(x.size(1)).to(x.device))) \n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.LinOut(x)\n",
    "        return x\n",
    "\n",
    "model = Transformer(vocab_size=vocab_size, emb_size=256, n_blocks=4, head_size=256, hidden_size=256, n_layers=2).to(device)\n",
    "torch.compile(model)\n",
    "print(f\"Millions of parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}\")\n",
    "\n",
    "steps = 400 # Tailored to be time-comparable to the MLP\n",
    "batch_size = 512\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.03) \n",
    "\n",
    "lossi = []\n",
    "vlossi = []\n",
    "\n",
    "for step in range(steps):\n",
    "    model.train()\n",
    "    indexes = torch.randint(0, len(inputs), (batch_size,))\n",
    "    \n",
    "    pred = model(inputs[indexes].to(device)) # everything in the forward pass happens in the model class\n",
    "    loss = F.cross_entropy(pred.view(-1, vocab_size), labels[indexes].to(device).view(-1))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        indexes = torch.randint(0, len(val_inputs), (batch_size//8,))\n",
    "        pred = model(val_inputs[indexes].to(device))\n",
    "        loss = F.cross_entropy(pred.view(-1, vocab_size), val_labels[indexes].to(device).view(-1))\n",
    "        vlossi.append(loss.item())\n",
    "\n",
    "    if step % (steps//10) == 0:\n",
    "        print(f\"Step {step:_>4d} - Loss: {lossi[-1]:.3f} - Val Loss: {vlossi[-1]:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 3))  \n",
    "plt.plot(vlossi[100:], label='Validation Loss')\n",
    "plt.plot(lossi[100:], label='Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
