{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import LucaM185"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/tinyShakespeare.txt\", \"r\") as f:\n",
    "    load = f.read()\n",
    "print(load[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longer sequences (20 chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texttoint = {elm: n for n, elm in enumerate(set(load))}\n",
    "inttotext = {n:elm for n, elm in enumerate(set(load))}\n",
    "dataset = [texttoint[c] for c in load]\n",
    "vocab_size = len(texttoint)\n",
    "\n",
    "sequence_length = 20\n",
    "\n",
    "sequences = torch.tensor([dataset[i:-sequence_length+i-1] for i in range(sequence_length+1)]).T\n",
    "inputs = sequences[:, :-1][2000:]\n",
    "labels = sequences[:, -1][2000:]\n",
    "val_inputs = sequences[:, :-1][:2000]\n",
    "val_labels = sequences[:, -1][:2000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the inputs\n",
    "\n",
    "We got a loss of 2.5 with the MLP on the last character, and looking at the example output... It couldnt really write complete words. We hope to do better by giving the model more context, other than preparing our dataset a little differently, we need to expand the input size of the first weight tensor (w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "\n",
    "w1 = torch.randn(vocab_size*sequence_length, hidden_size) * (6**0.5 / (vocab_size + hidden_size)**0.5)  # increased input size\n",
    "b1 = torch.zeros(hidden_size) \n",
    "w2 = torch.randn(hidden_size, vocab_size) * (6**0.5 / (vocab_size + hidden_size)**0.5)\n",
    "b2 = torch.zeros(vocab_size)\n",
    "\n",
    "params = [w1, b1, w2, b2]\n",
    "for p in params:\n",
    "    p.requires_grad_()\n",
    "\n",
    "steps = 2000\n",
    "batch_size = 256\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "lossi = []\n",
    "vlossi = []\n",
    "\n",
    "for step in range(steps):\n",
    "    indexes = torch.randint(0, len(inputs), (batch_size,))\n",
    "    \n",
    "    X = F.one_hot(inputs[indexes].long(), vocab_size).float() # this has shape (batch_size, sequence_length, vocab_size), but we want (batch_size, vocab_size*sequence_length)\n",
    "    X = X.view(batch_size, -1) # -1 means \"infer the size from the other dimensions\"\n",
    "    h1 = F.gelu(X @ w1 + b1)\n",
    "    pred = h1 @ w2 + b2\n",
    "    loss = F.cross_entropy(pred, labels[indexes])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "    if step % (steps//10) == 0:\n",
    "        print(f\"Step {step:_>4d} - Loss: {loss.item():.3f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X = F.one_hot(val_inputs.long(), vocab_size).float().view(len(val_inputs), -1)\n",
    "        h1 = F.gelu(X @ w1 + b1)\n",
    "        pred = h1 @ w2 + b2\n",
    "        vloss = F.cross_entropy(pred, val_labels)\n",
    "        vlossi.append(vloss.item())\n",
    "\n",
    "plt.figure(figsize=(6, 3))  \n",
    "plt.plot(lossi, label='Loss')\n",
    "plt.plot(vlossi, label='Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is taking a while, let's explore why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param, name in zip([w1, b1, w2, b2], [\"w1\", \"b1\", \"w2\", \"b2\"]):\n",
    "    # print variable name, shape and number of parameters\n",
    "    print(name, param.shape, param.numel())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer has too many parameters because it scales with **sequence_length × vocab_size × hidden_size**. Reducing **sequence_length** or **hidden_size** harms performance, but **vocab_size** can be improved. Using one-hot encoding is inefficient because it uses \\( \\text{vocab\\_size} \\) dimensions to represent small integers (e.g., 65 dimensions for a single character). One-hot guarantees orthogonality, but full orthogonality isn't always necessary.\n",
    "\n",
    "### Solution: Embeddings\n",
    "We replace one-hot encoding with **embeddings**, where each token (or character) is represented as a learned vector. This approach:\n",
    "\n",
    "- Is more parameter-efficient than one-hot encoding.\n",
    "- Operates in \\( O(1) \\), since embeddings involve simple tensor lookups.\n",
    "- Maps input tokens to a smaller, meaningful vector space.\n",
    "\n",
    "**Why embeddings work:**\n",
    "For words, embeddings capture semantic relationships. For example, \"king\" and \"queen\" may have similar embeddings, differing mainly in dimensions encoding gender. This efficient encoding allows embeddings to represent input data compactly while preserving essential distinctions.\n",
    "\n",
    "**Single characters:**\n",
    "Even for characters, embeddings work. While orthogonality is relaxed (not strict), learned embeddings distribute information effectively across dimensions, making them far more efficient than one-hot encodings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 16  # I chose this number of parameter embeddings, but you can try different values\n",
    "hidden_size = 128\n",
    "\n",
    "embeddings = torch.randn(vocab_size, emb_size) / 10  # Assigning a random vector to each character in the vocabulary (this will be trained)\n",
    "w1 = torch.randn(sequence_length*emb_size, hidden_size) * (6**0.5 / (vocab_size + hidden_size)**0.5)  \n",
    "b1 = torch.zeros(hidden_size) \n",
    "w2 = torch.randn(hidden_size, vocab_size) * (6**0.5 / (vocab_size + hidden_size)**0.5)\n",
    "b2 = torch.zeros(vocab_size)\n",
    "\n",
    "params = [embeddings, w1, b1, w2, b2]\n",
    "for p in params:\n",
    "    p.requires_grad_()\n",
    "\n",
    "steps = 2000\n",
    "batch_size = 256\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)  # dont forget to add embeddings to the optimizer\n",
    "lossi = []\n",
    "vlossi = []\n",
    "\n",
    "for step in range(steps):\n",
    "    indexes = torch.randint(0, len(inputs), (batch_size,))\n",
    "    \n",
    "    emb = embeddings[inputs[indexes]].view(batch_size, -1)\n",
    "    h1 = F.gelu(emb @ w1 + b1)\n",
    "    pred = h1 @ w2 + b2\n",
    "    loss = F.cross_entropy(pred, labels[indexes])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "    if step % (steps//10) == 0:\n",
    "        print(f\"Step {step:_>4d} - Loss: {loss.item():.3f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = embeddings[val_inputs].reshape(len(val_inputs), -1)\n",
    "        h1 = F.gelu(emb @ w1 + b1)\n",
    "        pred = h1 @ w2 + b2\n",
    "        vloss = F.cross_entropy(pred, val_labels)\n",
    "        vlossi.append(vloss.item())\n",
    "\n",
    "plt.figure(figsize=(6, 3))  \n",
    "plt.plot(lossi, label='Loss')\n",
    "plt.plot(vlossi, label='Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware that a very small number for the embedding size will hurt performance, but 16 doesnt seem too bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving to pytorch nn.Module for cleaner code and better initialization\n",
    "As you can see the model runs well, but now the complexity of the code is getting problematic, luckily pytorch has most of this already implemented. \n",
    "Just like we did before with the loss and the optimizer, here we can substitute the layers with pre made ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.w1 = nn.Linear(emb_size*sequence_length, hidden_size)\n",
    "        self.w2 = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        emb = self.embeddings(inputs).view(inputs.shape[0], -1)\n",
    "        h1 = F.gelu(self.w1(emb))\n",
    "        return self.w2(h1)\n",
    "    \n",
    "model = MLP(emb_size=16, hidden_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 2000\n",
    "batch_size = 256\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) \n",
    "\n",
    "lossi = []\n",
    "vlossi = []\n",
    "\n",
    "for step in range(steps):\n",
    "    indexes = torch.randint(0, len(inputs), (batch_size,))\n",
    "    \n",
    "    pred = model(inputs[indexes]) # everything in the forward pass happens in the model class\n",
    "    loss = F.cross_entropy(pred, labels[indexes])\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "    if step % (steps//10) == 0:\n",
    "        print(f\"Step {step:_>4d} - Loss: {loss.item():.3f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(val_inputs)\n",
    "        vloss = F.cross_entropy(pred, val_labels)\n",
    "        vlossi.append(vloss.item())\n",
    "\n",
    "plt.figure(figsize=(6, 3))  \n",
    "plt.plot(lossi, label='Loss')\n",
    "plt.plot(vlossi, label='Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPUs for extremely fast parallel execution\n",
    "\n",
    "As you can see the torch version works just as well, if not better and it's all around more refined\n",
    "\n",
    "Using nn.Module class from pytorch also allows us to execute the code on the GPU, which is way faster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")  # Set the device to GPU (cuda) \n",
    "model = MLP(emb_size=16, hidden_size=128).to(device)  # Send the model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10000\n",
    "batch_size = 256\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) \n",
    "\n",
    "lossi = []\n",
    "vlossi = []\n",
    "\n",
    "for step in range(steps):\n",
    "    indexes = torch.randint(0, len(inputs), (batch_size,))\n",
    "    \n",
    "    pred = model(inputs[indexes].to(device))  # Send the inputs to GPU\n",
    "    loss = F.cross_entropy(pred, labels[indexes].to(device))  # Send the labels to GPU\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "    if step % (steps//10) == 0:\n",
    "        print(f\"Step {step:_>4d} - Loss: {loss.item():.3f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(val_inputs.to(device))\n",
    "        vloss = F.cross_entropy(pred, val_labels.to(device))\n",
    "        vlossi.append(vloss.item())\n",
    "\n",
    "plt.figure(figsize=(6, 3))  \n",
    "plt.plot(lossi, label='Loss')\n",
    "plt.plot(vlossi, label='Validation Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the model is the same but everything happens in about a third of the time on my pc (ryzen 3100 and 3060 12GB) <br>\n",
    "Note: GPUs are often bandwith bound, not compute bound, so higher batch sizes play well with text, for example it may be that from 1 to 512 the training time stays basically the same, because the gpu is able to compute all of these in parallel and under a certain batch size you will not be taking advantage of this. <br>\n",
    "Now let's train it for a while and take a look at some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"             \\nKING RICHARD I\"\n",
    "print(string, end=\"\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(1000):\n",
    "        X = torch.tensor([texttoint[s] for s in string[-sequence_length:]]).long().view(1, -1).to(device)\n",
    "        pred = model.forward(X)\n",
    "        string += inttotext[torch.multinomial(F.softmax(pred, dim=1), 1).item()]\n",
    "        print(string[-1], end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is spitting out nosense but it clearly resembles the text it's trained on... Let's see how we can improve on this in part 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
