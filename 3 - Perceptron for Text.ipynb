{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare dataset\n",
    "Now we will move to a real world task, next-character prediction with the tiny-shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "with open(\"datasets/tinyShakespeare.txt\", \"r\") as f:\n",
    "    load = f.read()\n",
    "print(load[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texttoint = {elm: n for n, elm in enumerate(set(load))}\n",
    "inttotext = {n:elm for n, elm in enumerate(set(load))}\n",
    "dataset = [texttoint[c] for c in load]\n",
    "\n",
    "sequence_length = 1\n",
    "vocab_size = len(texttoint)\n",
    "inputs = torch.tensor(dataset[:-1])\n",
    "labels = torch.tensor(dataset[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "This complex dataset requires us to use something more elaborate, we will build on logistic regression to make an architecture that can deal with this kind of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.200290679931641\n",
      "5.199605941772461\n",
      "5.198920249938965\n",
      "5.198235511779785\n",
      "5.197550296783447\n",
      "5.196865081787109\n",
      "5.196181297302246\n",
      "5.195497035980225\n",
      "5.194812774658203\n",
      "5.194129943847656\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "m = torch.randn(vocab_size, vocab_size).requires_grad_()\n",
    "q = torch.randn(vocab_size).requires_grad_()\n",
    "\n",
    "# Training hyperparameters\n",
    "epochs = 10\n",
    "lr = 1e-2\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    pred = inputs @ m + q\n",
    "    loss = F.cross_entropy(pred, labels)\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        m -= lr * m.grad\n",
    "        q -= lr * q.grad\n",
    "        m.grad = None\n",
    "        q.grad = None\n",
    "    \n",
    "    if epoch % (epochs//10) == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is prohibitively slow, we cant train on this... Turns out that doing a forward and backward pass on the complete dataset is slow, let's see if we can make more approximate step by using a subset of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.801705360412598\n",
      "4.462411403656006\n",
      "4.155315399169922\n",
      "3.9009997844696045\n",
      "3.6736018657684326\n",
      "3.6835973262786865\n",
      "3.6150882244110107\n",
      "3.6090965270996094\n",
      "3.506439447402954\n",
      "3.3794827461242676\n"
     ]
    }
   ],
   "source": [
    "m = torch.randn(vocab_size, vocab_size).requires_grad_()\n",
    "q = torch.randn(vocab_size).requires_grad_()\n",
    "\n",
    "epochs = 10000  # training for 1000x more epochs \n",
    "lr = 1e-2\n",
    "batch_size = 1024  # taking a batch of 1024 samples at a time\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    indexes = torch.randint(0, len(inputs), (batch_size,))  # generating random indexes\n",
    "\n",
    "    pred = F.one_hot(inputs[indexes].long(), vocab_size).float() @ m + q\n",
    "    loss = F.cross_entropy(pred, labels[indexes])\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        m -= lr * m.grad\n",
    "        q -= lr * q.grad\n",
    "        m.grad = None\n",
    "        q.grad = None\n",
    "    \n",
    "    if epoch % (epochs//10) == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is cool, we can achieve a pretty decent loss, but some problems are way harder and can't be tackled with a solution this simple... <br>\n",
    "Remember what we did earlier? We expanded our logistic regressor to make a Perceptron, now we can try expanding the perceptron in the forward direction <br>\n",
    "This architecture should resemble a network of biological neurons, this is called Multi Layer Perceptron\n",
    "\n",
    "# MLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.357297897338867\n",
      "6.297913074493408\n",
      "4.232690811157227\n",
      "3.684781551361084\n",
      "3.6616287231445312\n",
      "3.2838339805603027\n",
      "3.1329193115234375\n",
      "3.067996025085449\n",
      "3.1219892501831055\n",
      "2.83640456199646\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "\n",
    "w1 = torch.randn(vocab_size, hidden_size, requires_grad=True)\n",
    "b1 = torch.randn(hidden_size, requires_grad=True)\n",
    "w2 = torch.randn(hidden_size, vocab_size, requires_grad=True)\n",
    "b2 = torch.randn(vocab_size, requires_grad=True)\n",
    "\n",
    "\n",
    "epochs = 10000\n",
    "lr = 1e-2\n",
    "batch_size = 1024\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    indexes = torch.randint(0, len(inputs), (batch_size,))\n",
    "    \n",
    "    X = F.one_hot(inputs[indexes].long(), vocab_size).float()\n",
    "    h1 = F.gelu(X @ w1 + b1)\n",
    "    pred = h1 @ w2 + b2\n",
    "    loss = F.cross_entropy(pred, labels[indexes])\n",
    "    lossi.append(loss.item())\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p in [w1, b1, w2, b2]:\n",
    "            p -= lr * p.grad\n",
    "            p.grad = None\n",
    "\n",
    "    if epoch % (epochs//10) == 0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding the architecture definetly made the model more effective but as we can see the starting loss is extremely high. <br>\n",
    "Let's figure that out why by printing some of the values from each layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALUES AT THE END OF A TRAINING RUN\n",
      "Input:  [0. 0. 0. 0. 0.]\n",
      "Hidden layer:  [-0.0958685  -0.00248625 -0.06264118 -0.06842674 -0.05632487]\n",
      "Output layer:  [-0.21774988  2.7743325  -1.9149207   3.3216686   0.7236721 ]\n",
      "Predictions:  [0.00061575 0.01227302 0.00011268 0.02122208 0.00157674]\n",
      "\n",
      "STARTING VALUES\n",
      "Input:  [0. 0. 0. 0. 0.]\n",
      "Hidden layer:  [ 7.5217730e-01 -6.5267354e-02  9.6309316e-01 -8.0141245e-04\n",
      " -5.4420985e-02]\n",
      "Output layer:  [ 2.5306673  7.3293037  4.9385695 24.713625  18.680634 ]\n",
      "Predictions:  [8.7072460e-11 1.0565784e-08 9.6742936e-10 3.7481460e-01 8.9892122e-04]\n"
     ]
    }
   ],
   "source": [
    "# todo convert to mean abs\n",
    "\n",
    "print(\"MEAN ABS VALUES AT THE END OF A TRAINING RUN\")\n",
    "print(\"Input: \", F.one_hot(inputs[indexes].long(), vocab_size).float()[0][:5].numpy()) # input\n",
    "print(\"Hidden layer: \", F.gelu(F.one_hot(inputs[indexes].long(), vocab_size).float()@w1+b1)[0][:5].detach().numpy()) # hidden layer\n",
    "print(\"Output layer: \", (F.gelu(F.one_hot(inputs[indexes].long(), vocab_size).float()@w1+b1)@w2+b2)[0][:5].detach().numpy()) # output layer\n",
    "print(\"Predictions: \", F.softmax(pred[0], dim=0)[:5].detach().numpy()) # predictions\n",
    "\n",
    "w1 = torch.randn(vocab_size, hidden_size, requires_grad=True)\n",
    "b1 = torch.randn(hidden_size, requires_grad=True)\n",
    "w2 = torch.randn(hidden_size, vocab_size, requires_grad=True)\n",
    "b2 = torch.randn(vocab_size, requires_grad=True)\n",
    "X = F.one_hot(inputs[indexes].long(), vocab_size).float()\n",
    "h1 = F.gelu(X @ w1 + b1)\n",
    "pred = h1 @ w2 + b2\n",
    "\n",
    "print(\"\\nMEAN ABS STARTING VALUES\")\n",
    "print(\"Input: \", F.one_hot(inputs[indexes].long(), vocab_size).float()[0][:5].numpy()) # input\n",
    "print(\"Hidden layer: \", F.gelu(F.one_hot(inputs[indexes].long(), vocab_size).float()@w1+b1)[0][:5].detach().numpy()) # hidden layer\n",
    "print(\"Output layer: \", (F.gelu(F.one_hot(inputs[indexes].long(), vocab_size).float()@w1+b1)@w2+b2)[0][:5].detach().numpy()) # output layer\n",
    "print(\"Predictions: \", F.softmax(pred[0], dim=0)[:5].detach().numpy()) # predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "By inspecting closely the values at the start of the training run, we clearly see that they are way too high (1-3 Orders of magnitude higher than the end of the run) <br>\n",
    "\n",
    "Now, let's explore out options: \n",
    "- We could obviously just ignore this problem and let the optimization deal with it, but that idea doesn't scale well with big models\n",
    "- We could simply reduce the starting values of weights and biases, and it's going to be better, but it's not the most elegant solution. Or even better we could normalize the starting values\n",
    "- We could normalize the values in each layer (suitable for deep networks) such that mean is zero and standard deviation is one\n",
    "\n",
    "Let's try out normalization of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.171732425689697\n",
      "3.6347289085388184\n",
      "3.336347818374634\n",
      "3.2786505222320557\n",
      "3.130432367324829\n",
      "3.1381890773773193\n",
      "3.124907970428467\n",
      "2.9968366622924805\n",
      "2.9734909534454346\n",
      "2.897439479827881\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "\n",
    "w1 = torch.randn(vocab_size, hidden_size, requires_grad=True)\n",
    "b1 = torch.randn(hidden_size, requires_grad=True)\n",
    "w2 = torch.randn(hidden_size, vocab_size, requires_grad=True)\n",
    "b2 = torch.randn(vocab_size, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for p in [w1, b1, w2, b2]:\n",
    "        p /= torch.norm(p, dim=0)\n",
    "\n",
    "epochs = 10000\n",
    "lr = 1e-2\n",
    "batch_size = 1024\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    indexes = torch.randint(0, len(inputs), (batch_size,))\n",
    "    \n",
    "    X = F.one_hot(inputs[indexes].long(), vocab_size).float()\n",
    "    h1 = F.gelu(X @ w1 + b1)\n",
    "    pred = h1 @ w2 + b2\n",
    "    loss = F.cross_entropy(pred, labels[indexes])\n",
    "    lossi.append(loss.item())\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p in [w1, b1, w2, b2]:\n",
    "            p -= lr * p.grad\n",
    "            p.grad = None\n",
    "\n",
    "    if epoch % (epochs//10) == 0:\n",
    "        print(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
